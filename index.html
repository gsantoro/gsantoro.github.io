
<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Styles -->
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    
    <!-- Meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">

    <!-- Favicon -->
    <link rel="shortcut icon" href="favicon.ico">
    
    <title>Giuseppe Santoro | CV</title>
</head>
<body>
    <div class="container">
  <div class="header">
  <h1 class="title">Giuseppe Santoro</h1>
  <h2 class="role">Senior Data Engineer</h2>
  
  <input type="checkbox" id="switch" name="mode" class="toggle-dark"></input>
  <label for="switch"><i id="moon" data-feather="moon" fill="var(--fg-color)"></i></label>
  
</div>
  <div class="contacts-block">
    <div class="sidebar">
      

<section class="contact">
    <div class="title">Contact</div>
    
    <div class="contact-item">
        <i data-feather="at-sign"></i>
        <a href="mailto:giuseppe.santoro@gmail.com">
            giuseppe.santoro@gmail.com
        </a>
    </div>
    

    
    <div class="contact-item">
        <i data-feather="phone"></i>
        <a href="#">
            07783542675
        </a>
    </div>
    

    
    <div class="contact-item">
        <i data-feather="github"></i>
        <a href="https://github.com/gsantoro">
            gsantoro
        </a>
    </div>
    

    
    <div class="contact-item">
        <i data-feather="linkedin"></i>
        <a href="https://linkedin.com/in/santorogiuseppe">
            santorogiuseppe
        </a>
    </div>
    

    

    

    
    <div class="contact-item">
        <i data-feather="feather"></i>
        <a href="https://www.mobygames.com/developer/giuseppe-santoro/credits/developerId,1021019/">
            giuseppe-santoro
        </a>
    </div>
    

    
    <div class="contact-item">
        <!-- <i data-icon="simple-icons:leetcode"></i> -->
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img"
            class="iconify iconify--simple-icons" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"
            data-icon="simple-icons:leetcode">
            <path
                d="M13.483 0a1.374 1.374 0 0 0-.961.438L7.116 6.226l-3.854 4.126a5.266 5.266 0 0 0-1.209 2.104a5.35 5.35 0 0 0-.125.513a5.527 5.527 0 0 0 .062 2.362a5.83 5.83 0 0 0 .349 1.017a5.938 5.938 0 0 0 1.271 1.818l4.277 4.193l.039.038c2.248 2.165 5.852 2.133 8.063-.074l2.396-2.392c.54-.54.54-1.414.003-1.955a1.378 1.378 0 0 0-1.951-.003l-2.396 2.392a3.021 3.021 0 0 1-4.205.038l-.02-.019l-4.276-4.193c-.652-.64-.972-1.469-.948-2.263a2.68 2.68 0 0 1 .066-.523a2.545 2.545 0 0 1 .619-1.164L9.13 8.114c1.058-1.134 3.204-1.27 4.43-.278l3.501 2.831c.593.48 1.461.387 1.94-.207a1.384 1.384 0 0 0-.207-1.943l-3.5-2.831c-.8-.647-1.766-1.045-2.774-1.202l2.015-2.158A1.384 1.384 0 0 0 13.483 0zm-2.866 12.815a1.38 1.38 0 0 0-1.38 1.382a1.38 1.38 0 0 0 1.38 1.382H20.79a1.38 1.38 0 0 0 1.38-1.382a1.38 1.38 0 0 0-1.38-1.382z"
                fill="currentColor"></path>
        </svg>
        <a href="https://leetcode.com/gsantoro">
            gsantoro
        </a>
    </div>
    
</section>

      <!-- 
 -->
    </div>

    <div class="profile-block">
      

<section class="career-profile">
    <h2 class="title">Career Profile</h2>
    <div class="details">Senior Data Engineer with 8 years of experience in Big Data including training and consultancy. 
Highlights include developing and supporting data pipelines at scale in various industries. Key interest in Distributed Systems and Linux Operating Systems.
</div>
</section>

    </div>
  </div>
  <div class="main-block">
    

<section class="experience">
    <h2 class="title">Experience</h2>
    
    <div class="experience-item">
        <div class="role">
            Big Data Engineer
        </div>
        <div class="company">
            Sony Interactive Entertainment (Industry: Gaming)
            <span class="time">October 2018 - August 2021</span>
        </div>
        
        <div class="team">
            Game Analytics Team at World Wide Studios
        </div>
        

        <ul>
            
            <li class="details">Wrote a standalone application to <strong>compress, deduplicate and re-partition</strong> historical and daily game analytics data. This made querying months worth of data in 10s of seconds possible. Previously, it would have taken hours to query a single month and it would have run out of memory for querying longer periods (<strong>Golang, AWS S3, AWS Athena, AWS Glue</strong>).</li>
            
            <li class="details">Wrote a Lambda to <strong>modify game analytics events in real-time</strong>. This lambda replaced IDs in the events according to custom rules per game and per event type stored in AWS S3. Part of this project included writing an in-memory cache for the S3 configs to reduce the processing time (<strong>Python, AWS Kinesis, AWS Lambda</strong>).</li>
            
            <li class="details">Wrote a Lambda to offload data from the real-time pipeline to the data warehouse (<strong>Golang, AWS Kinesis, AWS Kinesis Firehose, AWS S3</strong>)</li>
            
            <li class="details">Contributed to the successful launch and ongoing support of <strong>10+ Playstation first-party games</strong> from World Wide Studios by providing capacity estimations, cost optimizations, modifying configs, fixing bugs and being on call (<strong>AWS EMR, AWS Lambda, AWS Kinesis, Golang, Python, Terraform, Jenkins</strong>)</li>
            
            <li class="details">Contributed to a major <strong>refactoring of the data pipeline</strong> (<strong>Golang, Python, AWS Kinesis, EMR, AWS Lambda, Terraform</strong>)</li>
            
            <li class="details">Reduced the time taken by Engineers to manually create Grafana users, by writing an automation script that synchronized Okta users with Grafana users (<strong>Python</strong>)</li>
            
            <li class="details">Wrote a script that would allow Engineers from a Studio to <strong>search, download and transcode videos</strong> according to different criteria (<strong>Python</strong>).</li>
            
            <li class="details">Maintained and improved <strong>10+ internal microservices, 20+ EMR jobs, 5+ AWS Lambdas</strong> to support the data pipeline (Python, Golang, Terraform, Ansible)</li>
            
        </ul>
    </div>
    
    <div class="experience-item">
        <div class="role">
            Senior Software Engineer
        </div>
        <div class="company">
            Skimlinks (Industry: Advertising)
            <span class="time">March 2017 - August 2018</span>
        </div>
        
        <div class="team">
            Platform team
        </div>
        

        <ul>
            
            <li class="details">Reduced the running time of the daily pipeline by ~4x (from 8 to 2 hours), by optimizing some Hive queries and replacing the legacy code with a <strong>Google Dataflow job written in Java</strong> for the ingestion of 120 million documents into ElasticSearch.</li>
            
            <li class="details">Achieved <strong>30% cost savings in the infrastructure expenses</strong> of my team by replacing the legacy Hadoop cluster with a new setup with the same processing power plus Kerberos Security</li>
            
            <li class="details">Wrote a <strong>Google Dataflow job in Java</strong> as part of the migration of the data pipeline from Hive on-premise to Google BigQuery</li>
            
            <li class="details">Automated the scheduling of the data pipeline by creating, configuring and supporting an <strong>Airflow cluster running on Kubernetes on Google cloud</strong> and by writing various <strong>Airflow jobs in Python</strong></li>
            
            <li class="details">Improved various parts of the data pipeline (<strong>SQL for Hive queries, Java for Hive UDFs and Hive SerDes</strong>)</li>
            
            <li class="details">Developed scripts to collect and display data pipeline statistics into <strong>InfluxDB and Grafana</strong> (<strong>Python</strong>)</li>
            
        </ul>
    </div>
    
    <div class="experience-item">
        <div class="role">
            Big Data Engineer
        </div>
        <div class="company">
            BenevolentAI (Industry: Pharmaceutical research)
            <span class="time">July 2015 - February 2017</span>
        </div>
        
        <div class="team">
            Platform team
        </div>
        

        <ul>
            
            <li class="details">Reduced the ingestion time of 40 million scientific publications into <strong>ElasticSearch</strong> by ~8x (from 24 to 3 hours) by rewriting one of the steps of the data pipeline (<strong>Spark, Hadoop cluster on AWS</strong>).</li>
            
            <li class="details">Deployed, configured and maintained a variety of <strong>Hadoop, ElasticSearch and Cassandra</strong> clusters either bare-metal on-premise or on cloud (<strong>Python, Ansible, Amazon AWS</strong>)</li>
            
            <li class="details">Contributed to the migration of the batch data pipeline from a bare-metal Hadoop cluster to an Amazon EMR cluster by rewriting a Map/Reduce job that was written in Java into a <strong>Spark job written in Scala</strong></li>
            
            <li class="details">Automated the scheduling of the data pipeline by writing multiple <strong>Airflow</strong> jobs (<strong>Python</strong>)</li>
            
            <li class="details">Improved the data pipeline alerting capabilities (<strong>Python and Bash scripting</strong>)</li>
            
            <li class="details">Contributed and maintained various apps running on <strong>Mesos and Marathon</strong> as Docker containers</li>
            
        </ul>
    </div>
    
    <div class="experience-item">
        <div class="role">
            Senior Big Data Engineer
        </div>
        <div class="company">
            Big Data Partnership (Industry: Software) acquired by Teradata in 2016
            <span class="time">November 2013 - July 2015</span>
        </div>
        

        <ul>
            
            <li class="details">For 2 consecutive years, I delivered <strong>the official 5 days training course on Hadoop</strong> as part of the Hadoop Summit in partnership with Hortonworks</li>
            
            <li class="details">Delivered <strong>5+ training courses (duration 3-5 days) on Hadoop (either as Admin or Developer)</strong> at client offices across Europe</li>
            
            <li class="details">Installed, configured and optimized <strong>3 Hadoop clusters</strong> on bare-metal machines for different clients</li>
            
            <li class="details">Delivered <strong>5+ consultancy sessions</strong> on Hadoop and Big Data for various clients</li>
            
            <li class="details">Obtained <strong>3 Hortonworks certifications on Hadoop</strong> as Administrator (with grade 83.33/100), Developer (with grade 92/100) and Java Developer (with grade 93/100)</li>
            
            <li class="details"><strong>Lead Engineer</strong> in a team of 3 Engineers for a 4-month greenfield project for a client</li>
            
            <li class="details">Contributed to the development of the internal batch data pipeline (<strong>HBase, Pig, Hadoop</strong>)</li>
            
            <li class="details">Attended various training courses including <strong>Spark by Databricks, Administrator Training with Cassandra by Datastax, Developer Training with Cassandra by Datastax </strong></li>
            
        </ul>
    </div>
    
    <div class="experience-item">
        <div class="role">
            Software Engineer
        </div>
        <div class="company">
            VisualDNA (Industry: Advertising)
            <span class="time">January 2013 - October 2013</span>
        </div>
        
        <div class="team">
            Integrations Team
        </div>
        

        <ul>
            
            <li class="details">Developed 5+ integrations with external data providers via FTP protocol (<strong>Java</strong>)</li>
            
            <li class="details">Developed an internal RESTful API to serve data from a NoSQL database (<strong>Java, AWS SimpleDB</strong>)</li>
            
            <li class="details">Wrote various parts of the internal data pipeline (<strong>SQL, Hive, Pig</strong>)</li>
            
            <li class="details">Wrote a client for <strong>Kafka</strong> (<strong>Java</strong>)</li>
            
        </ul>
    </div>
    
</section>


    

<section class="education">
    <h2 class="title">Education</h2>
    
    <div class="education-item">
        <div class="degree">
            MSc in Computer Engineering
        </div>
        <div class="university">
            University of Catania, Italy
            <span class="time">September 2006 - July 2012</span>
        </div>
        <div class="grade">110/110 cum laude (with honours)</div>
    </div>
    
    <div class="education-item">
        <div class="degree">
            BSc in Computer Engineering
        </div>
        <div class="university">
            University of Palermo, Italy
            <span class="time">September 2002 - July 2006</span>
        </div>
        <div class="grade">110/110 cum laude (with honours)</div>
    </div>
    
</section>

    

<section class="skills">
    <h2 class="title">Skills</h2>
    
    <div class="skill-item">
        <span class="category">Backend</span>:
        <span class="details">Python, Golang, Java, SQL</span>
    </div>
    
    <div class="skill-item">
        <span class="category">Amazon AWS</span>:
        <span class="details">Lambda, Kinesis, Glue, Athena, EMR, S3</span>
    </div>
    
    <div class="skill-item">
        <span class="category">Google Cloud Platform</span>:
        <span class="details">Dataflow, BigQuery</span>
    </div>
    
    <div class="skill-item">
        <span class="category">Distributed Systems</span>:
        <span class="details">Spark, Hadoop, Hive, Pig, ElasticSearch</span>
    </div>
    
    <div class="skill-item">
        <span class="category">DevOps</span>:
        <span class="details">Docker, Terraform, Ansible, Fabric, Bash scripting</span>
    </div>
    
</section>

    

<section class="languages">
    <div class="title">Languages</div>
    
        <div class="idiom">Italian (Native)</div>
    
        <div class="idiom">English (Fluent)</div>
    
</section>

    <!-- 
 -->
    <!-- 
 -->
    <!-- 
 -->
  </div>
</div>
</body>
<script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
<script src="https://code.iconify.design/2/2.1.0/iconify.min.js"></script>

  <script src="/assets/javascript/toggleDark.js"></script>

<script>
  feather.replace()
</script>
</html>